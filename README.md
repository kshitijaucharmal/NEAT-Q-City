# ğŸ™ï¸ Evolving Deep Q-Learning for Sustainable Virtual City Management  
> **INC Competition @ PICT College**



https://github.com/user-attachments/assets/715f0cfa-25ee-43c6-874f-c19d476524db


![City Screenshot 1](https://github.com/user-attachments/assets/774b8fbc-e6a4-4d28-9e7a-2f16df34991a)

---

## ğŸ“˜ Overview

This project investigates the **synergy between Deep Q-Learning (DQL)** and **Neuro-Evolution of Augmenting Topologies (NEAT)** to manage a **dynamic virtual city simulation**. It was developed for the **INC competition at PICT College**.

Our goal is to evolve intelligent agents capable of **making sustainable decisions** for city growth â€” optimizing air quality, resource usage, and economic stability.

---

## ğŸ§  Technologies Used

- **Python 3**
- **Unity (custom environment)**
- **Numpy** â€“ for Deep Q-Network implementation

---

## ğŸŒ Core Features

âœ… **Simulated Urban Environment**  
âœ… **Deep Q-Learning Agents**  
âœ… **NEAT-based Topology Evolution**  
âœ… **Sustainability-Oriented Reward Function**  
âœ… **Dynamic Visualization & Logging**

---

## âš™ï¸ How It Works

### 1. ğŸ—ï¸ Simulated City Environment

A custom environment captures:
* ğŸ§â€â™‚ï¸ **Population Density**
* ğŸŒ«ï¸ **Pollution**
* ğŸï¸ **Recreation**
* ğŸ“š **Literacy Rate**
* ğŸš“ **Crime Rate**
* ğŸ’µ **Household Income**
* ğŸŒ³ **Green Space**
* ğŸ¥ **Healthcare**
* ğŸ’¼ **Employment Rate**
* ğŸŒ **Internet Coverage**


The environment returns **state vectors** and responds to **agent actions** like upgrading infrastructure, allocating resources, or introducing policies.

---

### 2. ğŸ¤– Deep Q-Learning Agent

Each agent:
- Uses a **neural network** to map states â†’ action-values (Q-values)
- Chooses actions with **Îµ-greedy** strategy
- Learns via **experience replay** and **Bellman updates**
- Is rewarded for decisions that lead to a more **sustainable and balanced city**

---

### 3. ğŸ§¬ NEAT for Evolution

NEAT evolves a **population of DQL agents** by:
- **Mutating network structures** (adding/removing nodes and connections)
- **Crossover** between top-performing agents
- Selecting agents with the **best reward scores** from simulation runs

This enables **dynamic exploration of network topologies** to discover better policies.

---

### 4. ğŸ“ˆ Evaluation & Visualization

Optional logging tools help track:
- Average reward per generation
- Performance trends over time
- Impact of evolved agents on air quality, economy, and resource usage
- Custom Environment in Unity used to visualise and simulate

---

## ğŸ“Š Sample Outputs

* ğŸ¥ **Building Hospital**
* ğŸŒ³ **Building Park**
* ğŸ­ **Building Factory**
* ğŸ›£ï¸ **Expand Roads**
* ğŸ« **Building Education Institutes**
* ğŸ˜ï¸ **Residential Building**
* ğŸ¢ **Building Offices**
* ğŸš“ **Building Police Station**
* ğŸ“¡ **Building Communication Tower**
* ğŸšœ **Building Farms**

Currently, the simulation can build these buildings, which are placed randomly in the city, but later,
the agent will be able to process a position for the buildings as well

---

## ğŸ Outcome

This hybrid DQL + NEAT approach:
- Adaptively **evolves agent architectures** and **training policies**
- Learns to **balance trade-offs** between air quality, economy, and resource usage
- Demonstrates the potential of **neuro-evolution in urban planning**

---

## ğŸ™Œ Acknowledgments

- **INC 2025 @ PICT College** for the platform  
- Research on **NEAT By Kenneth O. Stanley** for guiding our evolution strategy

---

## ğŸ“œ License

MIT License. See [LICENSE](LICENSE) for details.

---

> _â€œAI should not just optimize pixels â€” it should evolve policies that improve life, even in simulations.â€_
